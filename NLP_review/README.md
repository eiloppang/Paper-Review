# NLP 관련 논문

아직 NLP 자체에 대한 이해도가 많이 떨어져 있는 상태임. 이런 상태로는 최신 논문을 읽어도 이해를 하지 못하고, 겉핥기 식으로 읽게 됨.

최대한 해당 순서를 따라 읽어보고자 함. [참고한 사이트](https://asidefine.tistory.com/180)

### 읽고자 하는 순서

1. RNN  ➨ LSTM  ➨ GRU  ➨ Seq2Seq
2. Attention ➨ Transformer
3. Word2Vec ➨ GloVe ➨ FastText ➨ ELMo
4. GPT-1 ➨ BERT ➨ GPT-2 ➨ RoBERTa ➨ ALBERT ➨ ELECTRA
5. (추가적으로) Transformer-XL ➨ XLNet ➨ GPT-3
